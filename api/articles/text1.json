{"title":"神经网络","uid":"abeaebbd0b0f1648891139ac7999d458","slug":"text1","date":"2023-09-06T14:25:19.000Z","updated":"2023-09-10T09:12:30.645Z","comments":true,"path":"api/articles/text1.json","keywords":null,"cover":null,"content":"<p>深度学习</p>\n<h2 id=\"概念\"><a href=\"#概念\" class=\"headerlink\" title=\"概念\"></a>概念</h2><h3 id=\"pytorch、tensorflow-…\"><a href=\"#pytorch、tensorflow-…\" class=\"headerlink\" title=\"pytorch、tensorflow …\"></a>pytorch、tensorflow …</h3><p>  这些都是，深度学习的框架，而pytorch为动态，tensorflow为静态(这里就在对<strong>计算图</strong>进行讨论了)，静态图是把计算图构建好，包括其中的参数，然后输入数据根据参数得到输出；动态图则是先给出前面部分运算参数，根据输入值与前面参数的计算结果决定后面的图应该怎么构建。所谓框架呢，就是深度学习的一整套代码实现的一个流程。比如从输入一个数据到中间的计算，包括引入损失函数、学习率、反向传播、梯度下降、卷积、全链接……一系列流程，最终得到输出，这就是一个深度学习的过程，实现的代码就是框架。<br>pytorch是在tensor(张量)基础上开发的，所谓张量呢如图可直观明白，由于pytorch是在python上进行的，其中会用到torch库，在一些机器学习框架比如yolo中会遇到这些函数就是了（函数能实现的功能蛮多的，包括自动求导获取梯度，张量的创建和变换…）。![[Pasted image 20230629023312.png]]</p>\n<p>一般我们做深度学习都有一个概念，它是一个学习的过程，那肯定是有误差的，关于在真实值和误差之间，一般以线性回归方程来表示这个过程。</p>\n<blockquote><span class=\"custom-blockquote-svg\"><svg width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"\" xmlns=\"http://www.w3.org/2000/svg\" data-reactroot=\"\">\n<path fill=\"\" d=\"M22 12C22 6.5 17.5 2 12 2C6.5 2 2 6.5 2 12C2 17.5 6.5 22 12 22C13.8 22 15.5 21.5 17 20.6L22 22L20.7 17C21.5 15.5 22 13.8 22 12Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\" undefined=\"1\"></path>\n<path fill=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\" undefined=\"1\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M17 8.5C15.23 8.97 14.07 10.84 14.01 13.27C14 13.33 14 13.4 14 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M9 8.5C7.23 8.97 6.07 10.84 6.01 13.27C6 13.33 6 13.4 6 13.47V13.5\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M15.97 11.5H16.04C17.12 11.5 18 12.38 18 13.47V13.53C18 14.62 17.12 15.5 16.03 15.5H15.96C14.88 15.5 14 14.62 14 13.53V13.46C14 12.38 14.88 11.5 15.97 11.5Z\"></path>\n<path stroke-linejoin=\"round\" stroke-linecap=\"round\" stroke-miterlimit=\"10\" stroke-width=\"2\" stroke=\"\" d=\"M7.97 11.5H8.04C9.12 11.5 10 12.38 10 13.47V13.53C10 14.62 9.12 15.5 8.03 15.5H7.97C6.88 15.5 6 14.62 6 13.53V13.46C6 12.38 6.88 11.5 7.97 11.5Z\"></path>\n</svg>\n</span><p>通过分析因变量（y）与自变量（x）的变化关系，形如：$$y&#x3D;wx+b$$,我们需要的就是w和b的数值</p></blockquote>\n<p>我们所谓的<strong>损失函数</strong>就是预测值（$\\widehat{y}$）和真实值（y）之间的均方差。形如：$$\\frac{1}{m} \\sum_{i&#x3D;1}^{m}(y_i-\\widehat{y_i})^2$$<br>当采用<strong>梯度下降</strong>寻找最优时，此时参数会自己[根据当前的情况进行更新]，影响更新幅度的就是<strong>学习率</strong>，如我们令学习率为（lr）那么要求的参数w，b更新形如：$$w&#x3D;w-lr<em>w.grad （w.grad为函数关于参数w的偏导）$$$$b&#x3D;b-lr</em>b.grad$$<br>梯度下降，结合反向传播，在计算图中的使用情形类似；你扔进去一个X，然后给给Y，中间参数乱写，然后X会根据中间的计算得到输出，再结合对Y的比较进行中间参数的更新。学习率呢，会影响的时梯度下降时你下的幅度，会陷入局部最优和全局最优的情况。</p>\n<h2 id=\"计算图\"><a href=\"#计算图\" class=\"headerlink\" title=\"计算图\"></a>计算图</h2><p>对这样一个计算图，按照上面的说法，确定一个输入X&#x3D;2与已知结果Y&#x3D;6，其他为中间参数。而$a&#x3D;x+w，b&#x3D;w+1，y&#x3D;a*b$   是计算图的看法。对其求导则涉及到反向传播的内容</p>\n<p>![[Pasted image 20230629030506.png]]</p>\n<h3 id=\"反向传播（BP）\"><a href=\"#反向传播（BP）\" class=\"headerlink\" title=\"反向传播（BP）\"></a><strong>反向传播（BP）</strong></h3><p>它是干嘛的？我的理解，他就是对前向传播(通过一开的计算图[此时各参数是随机给定]和输入值得到输出值)得到的结果和真实值进行比较，然后做一个逆推，逆推的目的是通过求导然后得到各参数的值，并且进行一定程度上的调整，最终目的就是想让前向传播的输出值与真实值的误差(损失函数)尽可能的小</p>\n<p>通过求导进行反向的传导，其中会涉及到<strong>优化器</strong>，优化器在反向传播中指引各个参数调整到合适大小。而之前提到的梯度下降法（gradient descent），就是在反向传播中用到的一种优化器。此外还有随机梯度下降（SGD）和批量梯度下降（BGD）之间的差别在于更新参数的方程不同。<br>在上图中，对w的数值进行更新（先算出损失函数对应偏导数，根据链式法则，再带入优化器算法进行参数值更新）：$$ΔE(w_t)&#x3D;\\frac{δy}{δw}&#x3D;\\frac{δy}{δa}\\frac{δa}{δw}+\\frac{δy}{δb}\\frac{δb}{δy}&#x3D;b<em>1+a</em>1…、（E为损失函数）$$<br>学习率：lr<br>GD：$w_{t+1}&#x3D;w_t-lr<em>ΔE(w_t)$<br>BGD：$w_{t+1}&#x3D;w_t-lr</em>\\sum_{i&#x3D;1}^{n}ΔE(w_t，x_i，y_i)、（x_i,y_i为当前网络层的输入和输出）$<br>SGD:$w_{t+1}&#x3D;w_t-lr*g_t、（g_t&#x3D;ΔE(w_t，x_t，y_t)，从n个样本中随机选择一个样本）$<br>此外还有其他的一些优化器方法（也是蛮多的）</p>\n<h3 id=\"逻辑回归（logistic-regression）\"><a href=\"#逻辑回归（logistic-regression）\" class=\"headerlink\" title=\"逻辑回归（logistic regression）\"></a>逻辑回归（logistic regression）</h3><p>不同于之前的y与x线性回归的关系，逻辑回归一般是用来解决二分类模型的，比较常见的函数有sigmod函数，形如：$$y&#x3D;f(z)&#x3D;\\frac{1}{1+e^{-z}}，z&#x3D;wx+b$$<br>$$result&#x3D;\\begin{cases}<br>0,&amp; \\text{y$\\leq$0.5}\\<br>1,&amp; \\text{y&gt;0.5}<br>\\end{cases}</p>\n","text":"深度学习 概念pytorch、tensorflow … 这些都是，深度学习的框架，而pytorch为动态，tensorflow为静态(这里就在对计算图进行讨论了...","link":"","photos":[],"count_time":{"symbolsCount":"2k","symbolsTime":"2 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A6%82%E5%BF%B5\"><span class=\"toc-text\">概念</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#pytorch%E3%80%81tensorflow-%E2%80%A6\"><span class=\"toc-text\">pytorch、tensorflow …</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E8%AE%A1%E7%AE%97%E5%9B%BE\"><span class=\"toc-text\">计算图</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88BP%EF%BC%89\"><span class=\"toc-text\">反向传播（BP）</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88logistic-regression%EF%BC%89\"><span class=\"toc-text\">逻辑回归（logistic regression）</span></a></li></ol></li></ol>","author":{"name":"shol","slug":"blog-author","avatar":"http://localhost:4000/static/img/d4.jpg","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"ROS","uid":"d1785060d540402d26f62361462e606a","slug":"ROS","date":"2023-09-06T14:25:57.000Z","updated":"2023-09-22T08:35:50.813Z","comments":true,"path":"api/articles/ROS.json","keywords":null,"cover":[],"text":" 用途ROS类似一个数据传输的管道,也提供接口,好比cpu和其他设备间信息交互的总线.一开始是为解决机器人上信息传输问题,因而目前ROS开发社区中对这一块的库可...","link":"","photos":[],"count_time":{"symbolsCount":"1.7k","symbolsTime":"2 mins."},"categories":[],"tags":[],"author":{"name":"shol","slug":"blog-author","avatar":"http://localhost:4000/static/img/d4.jpg","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Hello World","uid":"b9663f58f18133b35bfe243f3e916a80","slug":"hello-world","date":"2023-09-06T14:22:00.639Z","updated":"2023-09-06T14:22:00.643Z","comments":true,"path":"api/articles/hello-world.json","keywords":null,"cover":null,"text":"Welcome to Hexo! This is your very first post. Check documentation for more info...","link":"","photos":[],"count_time":{"symbolsCount":430,"symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"shol","slug":"blog-author","avatar":"http://localhost:4000/static/img/d4.jpg","link":"/","description":"","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}